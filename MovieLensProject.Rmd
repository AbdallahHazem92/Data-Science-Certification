---
title: "MovieLensProject"
author: "Abdallah Hazem"
date: "5/10/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE, echo=FALSE}
# Install all needed libraries if it is not present

if(!require(ggplot2)) install.packages("ggplot2")
if(!require(stringr)) install.packages("stringr")
if(!require(forcats)) install.packages("forcats")
if(!require(kableExtra)) install.packages("kableExtra")
if(!require(tidyr)) install.packages("tidyr")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(anytime)) install.packages("anytime", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")

```

```{r, include=FALSE, echo=FALSE}
# Loading all needed libraries
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidyr)
library(stringr)
library(forcats)
library(kableExtra)
library(caret)
library(anytime)
library(tidyverse)
library(lubridate)

```

```{r, include=FALSE, echo=FALSE}
#############################################################
# Create edx set, validation set, and submission file
#############################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

#**1- Introduction**

You will find here a document that related to the MovieLens Project of the HervardX: PH125.9x Data Science: Capstone course. The present report start with a general idea of the project and by representing its objectif. The purpose for this project is creating a recommender system using MovieLens dataset.Recommender systems are a class of statistical learning systems that analyse individual
past choices and/or preferences to propose relevant information to make future choices.This document contains an exploratory analysis section in which some characteristics of the data set are shown. This section will also explain the process, techniques and methods that were used to handle the data and to create the predicive model.

#**1-1 Aim of the project**

The aim of this Project is to create a recommendations system by machine learning algorithm that predicts user ratings (from 0.5 to 5 stars) using the inputs of a provided subset  (edx dataset provided by the staff) to predict movie ratings in a provided validation set.

#**2- Explore Dataset**
#**2-1 Data componant **
The MovieLens dataset is automatically downloaded
The data provided is a list of ratings made by anonymised users of a number of movies. The entire training dataset is a table of 9000055 rows and 6 variables. Note that the dataset is extermely sparse:if each user had rated each movie, the dataset should contain 54000330 ratings, i.e. 85 times more.

**The 6 varibales are :**

userId: Unique identification number given to each user. numeric variable
• movieId: Unique identification number given to each movie. numeric variable.
• timestamp: Code that contains date and time in what the rating was given by the user to the specific movie. integer variable.
• title: Title of the movie. character variable.
• genres: Motion-picture category associated to the film. character variable.
• rating: Rating given by the user to the movie. From 0 to 5 stars in steps of 0.5. numeric variable.
```{r, echo=FALSE, include=TRUE}

edx$userId <- as.factor(edx$userId) # Convert `userId` to `factor`.
edx$movieId <- as.factor(edx$movieId) # Convert `movieId` to `factor`.
edx$genres <- as.factor(edx$genres) # Convert `genres` to `factor`.
edx$timestamp <- as.POSIXct(edx$timestamp, origin = "1970-01-01") # Convert `timestamp to `POSIXct`.
head(edx) %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)
```

#**2-2 Dataset Pre-Processing and Feature Engineering**

#**2-2-3 The pre-processing phase is composed by this steps:**

1. Convert ```timestamp``` to a human readable date format;
2. Extract the month and the year from the date;
3. Extract the release year for each movie from the title;

```{r, echo=FALSE, include=TRUE}
edx <- edx %>%
  mutate(title = str_trim(title)) %>%
  extract(title,
          c("titleTemp", "release"),
          regex = "^(.*) \\(([0-9 \\-]*)\\)$",
          remove = F) %>%
  mutate(release = if_else(str_length(release) > 4,as.integer(str_split(release, "-",
                                                simplify = T)[1]),as.integer(release))) %>%
  mutate(title = if_else(is.na(titleTemp),title,titleTemp)) %>%
  select(-titleTemp)

# validation dataset

validation <- validation %>%
  mutate(title = str_trim(title)) %>%
  extract(title,
          c("titleTemp", "release"),
          regex = "^(.*) \\(([0-9 \\-]*)\\)$",
          remove = F) %>%
  mutate(release = if_else(str_length(release) > 4,
                           as.integer(str_split(release, "-",simplify = T)[1]),
                           as.integer(release))) %>%
  mutate(title = if_else(is.na(titleTemp),title,titleTemp)) %>%
  select(-titleTemp)
```

```{r, echo=FALSE, include=TRUE}
head(edx) %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)
```

#**2-3 Ratings distribution**

```{r, echo=FALSE, include=TRUE}
vec_ratings <- as.vector(edx$rating)
vec_ratings <- vec_ratings[vec_ratings != 0]
vec_ratings <- factor(vec_ratings)
qplot(vec_ratings) +
  ggtitle("Ratings' Distribution")
```

The above rating distribution shows that the users have a general tendency to rate movies between 3 and 4. This is a very general conclusion. We should further explore the effect of different features to make a good predictive model.

```{r, echo=FALSE, include=TRUE}
edx %>% group_by(movieId , release)%>% summarise(Average_Rating=mean(rating))%>%
  select(release, movieId, Average_Rating) %>% 
  ggplot(aes(release, Average_Rating)) + 
  geom_point(alpha = 0.50,color = "blue" ) +
  geom_smooth(color = "red")
```

• There is clearly an effect where the average rating goes down. More striking is that recent movies are more likely to receive a bad rating, where the variance of ratings for movies before the early seventies is MUch lower.

• Very early years: very few ratings (very pale colour) possibly since fewer people decide to watch older movies.
• Early years: Strong effect where many ratings are made when the movie is first screen, then very quiet period.
• Recent years: More or less constant colour.

###2-3-1 Movies distribution by Average Rating

```{r, echo=FALSE, include=TRUE}
edx%>% group_by(movieId)%>% summarise(Average_Rating=mean(rating))%>%
  ggplot(aes(Average_Rating))+geom_histogram(bins=30)+
  ggtitle("Movies by average rating")+theme_bw()+
  geom_vline(xintercept = mean(edx$rating),color="black")  + 
  scale_x_continuous(breaks=c(1,2,3,mean(edx$rating),4,5), labels=c("1","2","3","mean","4","5")) 

```

###2-3-2 Users distribution by Average Rating

```{r, echo=FALSE, include=TRUE}
edx%>% group_by(userId)%>% summarise(Average_Rating=mean(rating))%>%
  ggplot(aes(Average_Rating))+geom_histogram(bins=30)+
  ggtitle("Users by average rating")+theme_bw()+
  geom_vline(xintercept = mean(edx$rating),color="black") + 
  scale_x_continuous(breaks=c(1,2,3,mean(edx$rating),4,5), labels=c("1","2","3","mean","4","5")) 

```

Movies and users distribution by Average Rating look “Normal”. We can see that there is movie and user effects/bias, as “good” movies tempt to be rated higher than others, as well as some users are more easygoing than others.

##2-4 Ratings distribution by count

```{r, echo=FALSE, include=TRUE}
edx %>% group_by(movieId)%>% summarise(Average_Rating=mean(rating), Counts = n())%>%
  select(Counts, movieId, Average_Rating) %>% 
  ggplot(aes(Counts, Average_Rating)) + 
  geom_point(alpha = 0.50,color = "blue" ) +
  geom_smooth(color = "red")
```

If a movie is very good, many people will watch it and rate it. In other words, we should see some correlation between ratings and numbers of ratings. Again, some sort of rescaling of time, logarithmic or other, need considering.
The effect of good movies attracting many spectators is noticeable. It is also very clear that movies with few spectators generate extremely variable results.

#**3- The Model**

#**3-1 Crating Train and Test set**

First we wust create the train set and test set.

```{r , echo=TRUE, include=TRUE, warning=FALSE}
edx <- edx %>% select(userId, movieId, rating)

test_index <- createDataPartition(edx$rating, times = 1, p = .2, list = F)
    # Create the index

  train <- edx[-test_index, ] # Create Train set
  test <- edx[test_index, ] # Create Test set
  test <- test %>% # The same movieId and usersId appears in both set. (Not the same cases)
    semi_join(train, by = "movieId") %>%
    semi_join(train, by = "userId")
```

```{r , echo=FALSE, include=FALSE, warning=FALSE}
dim(train)
dim(test)
```


#**3-2 Creating Baseline **

We will generate basic model which consider the most common rating from the train set to be predicted into the test set. This is the baseline model.  

```{r , echo=FALSE, include=FALSE, warning=FALSE}
MU_hat <- mean(train$rating) # Mean accross all movies.
RMSE_baseline <- RMSE(test$rating, MU_hat) # RMSE in test set.
RMSE_baseline
```  

Now, we have the RMSE to be *beaten* by our model.  

```{r , echo=FALSE, include=TRUE, warning=FALSE}
TableOfRmse <- data_frame(Method = "Baseline", RMSE = RMSE_baseline)
TableOfRmse %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)
```  

We can observe that the RMSE of the most basic model is `r RMSE_baseline`. It's bigger than 1! In this context, this is a very bad model.

#**3-3 User and Movie effect Model**

We are trying to get a better RMSE by considering the user effect and the movie effect as predictors. 

$$\hat{y}_i=UI+MI+\varepsilon$$
```{r , echo=FALSE, include=FALSE, warning=FALSE}
MU <- mean(train$rating)

MovieAvarages <- train %>%
  group_by(movieId) %>%
  summarize(MI = mean(rating - MU))

UserAvarages <- test %>%
  left_join(MovieAvarages, by = "movieId") %>%
  group_by(userId) %>%
  summarize(UI = mean(rating - MU - MI))

RatingsOfPredicted <- test %>%
  left_join(MovieAvarages, by = "movieId") %>%
  left_join(UserAvarages, by = "userId") %>%
  mutate(pred = MU + MI + UI) %>% .$pred

RmseModel <- RMSE(RatingsOfPredicted, test$rating)
RmseModel  
```  

```{r , echo=FALSE, include=TRUE, warning=FALSE}
TableOfRmse <- rbind(TableOfRmse,
                    data_frame(Method = "User & Movie Effect", RMSE = RmseModel))

TableOfRmse %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)
```  

We've got obtained a better RMSE so we can make predictions on Validation data.

#**3-4 User and Movie effect Model on validation data**

Validation data set needs to be handled the same as the train data set was handled after that we will be apple to make predictions.


```{r , echo=FALSE, include=FALSE, warning=FALSE}
validation <- validation %>% select(userId, movieId, rating)

validation$userId <- as.factor(validation$userId)
validation$movieId <- as.factor(validation$movieId)

validation <- validation[complete.cases(validation), ]
```


```{r , echo=FALSE, include=FALSE, warning=FALSE}
MU <- mean(train$rating)

MovieAvarages <- train %>%
  group_by(movieId) %>%
  summarize(MI = mean(rating - MU))

UserAvarages <- test %>%
  left_join(MovieAvarages, by = "movieId") %>%
  group_by(userId) %>%
  summarize(UI = mean(rating - MU - MI))

RatingsOfPredicted <- test %>%
  left_join(MovieAvarages, by = "movieId") %>%
  left_join(UserAvarages, by = "userId") %>%
  mutate(pred = MU + MI + UI) %>% .$pred
```
```{r , echo=FALSE, include=FALSE, warning=FALSE}
PredictedValues <- validation %>%
  left_join(MovieAvarages, by = "movieId") %>%
  left_join(UserAvarages, by = "userId") %>%
  mutate(pred = MU + MI + UI) %>% .$pred

val_RMSE <- RMSE(PredictedValues, validation$rating, na.rm = T)
val_RMSE
```  
```{r , echo=FALSE, include=TRUE, warning=FALSE}
RmseTableVal <- data_frame(Method = "User & Movie Effect on validation", RMSE = val_RMSE)
RmseTableVal %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)
```  

We can see above that this RMSE is higher than the RMSE on the test set. This is highly probable, given that this is unseeing data. The good thing is that the difference is just `r val_RMSE - RmseModel`. Now, let's see if *regularisation* give us better results.

#**3-5 Getting Regularisation **

Regularisation will evaluate the  various values for lambda to give us the corresponding RMSE.

```{r, echo=FALSE, include=TRUE , warning=FALSE}
ValuesOfLambda <- seq(0, 7, .2)

RmseFunctionReg <- sapply(ValuesOfLambda, function(l){
  
  MU <- mean(train$rating)
  
  MI <- train %>%
    group_by(movieId) %>%
    summarize(MI = sum(rating - MU)/(n()+l))
  
  UI <- train %>%
    left_join(MI, by="movieId") %>%
    group_by(userId) %>%
    summarize(UI = sum(rating - MI - MU)/(n()+l))
  
  RatingsOfPredicted <- test %>%
    left_join(MI, by = "movieId") %>% 
    left_join(UI, by = "userId") %>%
    mutate(pred = MU + MI + UI) %>% .$pred
  
  return(RMSE(RatingsOfPredicted, test$rating))
})

qplot(ValuesOfLambda, RmseFunctionReg,
      main = "Regularisation",
      xlab = "RMSE", ylab = "Lambda") # lambda vs RMSE

LambdaOpt <- ValuesOfLambda[which.min(RmseFunctionReg)]
LambdaOpt # Lambda which minimizes RMSE
```  

```{r, echo=FALSE, include=TRUE, warning=FALSE}
TableOfRmse <- rbind(TableOfRmse,
                    data_frame(Method = "User & Movie Effect Regularisation",
                               RMSE = min(RmseFunctionReg)))
TableOfRmse %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)
```  

The regularisation give as a higher RMSE than the first "User & Movie Effect" model. This is unexpected

#**3-6 Getting Regularisation on validation data set**

let's see what is the result and performance of the regularisation on the validation data .

```{r, echo=FALSE, include=TRUE, warning=FALSE}
FuncRmseRegValue <- sapply(ValuesOfLambda, function(l){
  
  MU <- mean(train$rating)
  
  MI <- train %>%
    group_by(movieId) %>%
    summarize(MI = sum(rating - MU)/(n()+l))
  
  UI <- train %>%
    left_join(MI, by="movieId") %>%
    group_by(userId) %>%
    summarize(UI = sum(rating - MI - MU)/(n()+l))
  
  PredictedRegValue <- validation %>%
    left_join(MI, by = "movieId") %>% 
    left_join(UI, by = "userId") %>%
    mutate(pred = MU + MI + UI) %>% .$pred
  
  return(RMSE(PredictedRegValue, validation$rating, na.rm = T))
})

qplot(ValuesOfLambda, FuncRmseRegValue,
      main = "Regularisation on validation data set",
      xlab = "Lambda", ylab = "RMSE")

LambdaRegOpt <- ValuesOfLambda[which.min(FuncRmseRegValue)]
LambdaRegOpt # Lambda which minimizes RMSE
RmseMin <- min(FuncRmseRegValue) # Best RMSE
RmseMin
```  

**RMSE IS **

```{r, echo=FALSE, include=TRUE , warning=FALSE}
RmseTableVal <- rbind(RmseTableVal,
                    data_frame(Method = "User & Movie Effect Reg. on validation",
                               RMSE = min(FuncRmseRegValue)))
RmseTableVal %>%
   kable() %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                 position = "center",
                 font_size = 10,
                 full_width = FALSE)
```  

#**4- Results**
```{r, echo=FALSE, include=TRUE, warning=FALSE}
rbind(TableOfRmse, RmseTableVal) %>% knitr::kable(caption = "RMSEs Summary")
```  

We can observe that the better RMSE is obtained from the User & Movie Effect model. However, this RMSE only obtained on the test set. When we move to the validation data set, we obtain the worse RMSE (ignoring the baseline).

Considering that we MUst trust more in the performance of the model when we predict from unseeing data, we can say that the RMSE that results from the *User & Movie Effect with Regularisation on validation* (the last line in the table above) is our definitive model. This RMSE is obtained when $\lambda=5$ which permit us to achieve **RMSE equal to `r RmseMin`.**

#**5- Conclusion**
The variables userId and movieId have sufficient predictive power to permit us to predict how a user will
rate a movie. This tell us that we could make better recommendations about movie to specific users of the
streaming service. Therefore, the user could decide to spend more time using the service.
The RMSE  is pretty acceptable considering that we have few predictors, but both User
and Movie effects are power enough to predict the rating that will be given to a movie, by a specific user.